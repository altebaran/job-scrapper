name: ğŸ¯ Daily Job Scrape

on:
  # â”€â”€ Runs every day at 06:00 UTC (07:00 CET / 08:00 CEST) â”€â”€
  schedule:
    - cron: "0 6 * * *"

  # â”€â”€ Manual trigger from GitHub UI â”€â”€
  workflow_dispatch:
    inputs:
      reset_seen:
        description: "Reset seen jobs database (show all jobs as new)"
        required: false
        default: "false"
        type: choice
        options:
          - "false"
          - "true"

# â”€â”€ Permissions: read code, write to push seen_jobs back, deploy Pages â”€â”€
permissions:
  contents: write
  pages: write
  id-token: write

jobs:
  scrape:
    runs-on: ubuntu-latest
    timeout-minutes: 15

    steps:
      # â”€â”€ 1. Checkout repo (with history for pushing back) â”€â”€
      - name: ğŸ“¥ Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      # â”€â”€ 2. Setup Python â”€â”€
      - name: ğŸ Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"
          cache: "pip"

      # â”€â”€ 3. Install dependencies â”€â”€
      - name: ğŸ“¦ Install dependencies
        run: pip install -r requirements.txt

      # â”€â”€ 4. Optional: reset seen database â”€â”€
      - name: ğŸ”„ Reset seen database
        if: github.event.inputs.reset_seen == 'true'
        run: |
          echo '{"seen": {}, "last_run": null}' > data/seen_jobs.json
          echo "âœ… Seen jobs database reset"

      # â”€â”€ 5. Run the scraper â”€â”€
      - name: ğŸ” Run job scraper
        run: python scraper.py
        env:
          GITHUB_ACTIONS: "true"

      # â”€â”€ 6. Send email notification â”€â”€
      - name: ğŸ“§ Send email report
        if: hashFiles('reports/latest-report.html') != ''
        uses: dawidd6/action-send-mail@v3
        with:
          server_address: ${{ secrets.SMTP_SERVER }}
          server_port: ${{ secrets.SMTP_PORT }}
          username: ${{ secrets.EMAIL_USERNAME }}
          password: ${{ secrets.EMAIL_PASSWORD }}
          subject: "ğŸ¯ Job Report â€” ${{ env.REPORT_DATE }} â€” ${{ env.JOB_COUNT }} new matches"
          to: ${{ secrets.EMAIL_TO }}
          from: ${{ secrets.EMAIL_USERNAME }}
          html_body: file://reports/latest-report.html
          ignore_cert: true
        continue-on-error: true  # Don't fail the workflow if email fails

      # â”€â”€ 7. Commit updated seen_jobs.json + reports back to repo â”€â”€
      - name: ğŸ’¾ Persist state & reports
        run: |
          git config user.name "Job Scraper Bot"
          git config user.email "bot@jobscraper.local"
          git add data/seen_jobs.json
          git add reports/
          git add docs/
          # Only commit if there are changes
          git diff --staged --quiet || git commit -m "ğŸ¤– Daily scrape â€” $(date +%Y-%m-%d) â€” ${JOB_COUNT:-0} new jobs"
          git push
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

      # â”€â”€ 8. Upload report as downloadable artifact â”€â”€
      - name: ğŸ“ Upload report artifact
        if: hashFiles('reports/latest-report.html') != ''
        uses: actions/upload-artifact@v4
        with:
          name: job-report-${{ env.REPORT_DATE }}
          path: |
            reports/latest-report.html
            reports/latest-report.md
          retention-days: 30

  # â”€â”€ Deploy reports to GitHub Pages â”€â”€
  deploy-pages:
    needs: scrape
    runs-on: ubuntu-latest
    if: always()

    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}

    steps:
      - name: ğŸ“¥ Checkout (after scrape pushed)
        uses: actions/checkout@v4
        with:
          ref: main

      - name: ğŸ“„ Setup Pages
        uses: actions/configure-pages@v4

      - name: ğŸ“¦ Upload Pages artifact
        uses: actions/upload-pages-artifact@v3
        with:
          path: docs

      - name: ğŸš€ Deploy to GitHub Pages
        id: deployment
        uses: actions/deploy-pages@v4
